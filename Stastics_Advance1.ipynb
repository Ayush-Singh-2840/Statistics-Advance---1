{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "817517d1-9d31-489e-bc0e-27e07e07bc8a",
      "cell_type": "markdown",
      "source": "##Question.1\n\n# Properties of the F-distribution\n\nThe F-distribution is a continuous probability distribution commonly used in analysis of variance (ANOVA) and hypothesis testing to compare sample variances.\n\n## 1. Definition\n- The F-distribution is the ratio of two independent chi-squared distributions, each divided by its degrees of freedom.\n- It is defined by two parameters: numerator degrees of freedom and denominator degrees of freedom.\n\n## 2. Shape\n- The F-distribution is right-skewed, and the skewness decreases as the degrees of freedom increase.\n\n## 3. Range\n- It is defined only for positive values, ranging from 0 to infinity.\n\n## 4. Mean\n- The mean exists only when the denominator degrees of freedom is greater than 2. Otherwise, the mean is undefined.\n\n## 5. Variance\n- The variance exists only when the denominator degrees of freedom is greater than 4. Otherwise, the variance is undefined.\n\n## 6. Skewness and Kurtosis\n- The skewness and kurtosis depend on the degrees of freedom. As the degrees of freedom increase, the distribution becomes more symmetric.\n\n## 7. Application in Hypothesis Testing\n- It is used in ANOVA to compare variances of two populations.\n\n## 8. Relationship with Other Distributions\n- The F-distribution approaches a normal distribution as the degrees of freedom for both the numerator and the denominator increase.\n",
      "metadata": {}
    },
    {
      "id": "9746f7c7-c24e-40eb-96ff-166759e52c5f",
      "cell_type": "markdown",
      "source": "##Question.2\n\n# Usage of the F-distribution in Statistical Tests\n\nThe F-distribution is commonly used in several types of statistical tests, primarily when comparing variances. Here are the key tests where the F-distribution is used and why it is appropriate:\n\n## 1. Analysis of Variance (ANOVA)\n- **Purpose**: ANOVA is used to compare the means of three or more groups to determine if there are any statistically significant differences between the group means.\n- **Why F-distribution**: The F-distribution is used to test the ratio of the variances between groups to the variances within groups. If the variance between the groups is significantly larger than within the groups, it suggests that the means are different.\n\n## 2. Regression Analysis\n- **Purpose**: In multiple regression analysis, the F-test is used to determine if the overall regression model is a good fit for the data.\n- **Why F-distribution**: It compares the variance explained by the model to the unexplained variance. A significant F-statistic indicates that the model explains a substantial portion of the variance in the dependent variable.\n\n## 3. F-test for Equality of Two Variances\n- **Purpose**: This test is used to compare the variances of two independent samples to check if they come from populations with the same variance.\n- **Why F-distribution**: The F-distribution is appropriate because it models the ratio of two sample variances, helping to determine if they are statistically different.\n\n## 4. MANOVA (Multivariate Analysis of Variance)\n- **Purpose**: MANOVA extends ANOVA by allowing for the analysis of multiple dependent variables simultaneously.\n- **Why F-distribution**: It uses the F-distribution to test the multivariate effect of the independent variable(s) on the dependent variables.\n\n## Why F-distribution is Appropriate\n- The F-distribution is designed to handle ratios of variances, making it suitable for tests that involve comparing variability across groups or models.\n- Its skewed shape allows for sensitive detection of differences, particularly when the sample sizes are small.\n\n",
      "metadata": {}
    },
    {
      "id": "f7b2bbb1-4439-40a1-9ce1-4bef48a754bc",
      "cell_type": "markdown",
      "source": "##Question.3\n\n\n# Key Assumptions for Conducting an F-test to Compare Variances\n\nWhen conducting an F-test to compare the variances of two populations, several key assumptions need to be met for the test to be valid. These assumptions are:\n\n## 1. **Independence of Samples**\n- The two samples must be independent of each other. This means that the selection of one sample should not influence the selection of the other.\n\n## 2. **Normality of Populations**\n- Both populations from which the samples are drawn should follow a normal distribution. The F-test is sensitive to departures from normality, and the results may not be reliable if this assumption is violated.\n\n## 3. **Random Sampling**\n- The samples must be randomly selected from the populations. This ensures that the samples are representative of the populations.\n\n## 4. **Equality of Measurement Scale**\n- The data in both samples should be measured on the same scale. This ensures that the variances are comparable.\n\n## 5. **Non-Zero Variances**\n- Both populations should have non-zero variances. The F-test cannot be conducted if either population has a variance of zero.\n\n## Why These Assumptions Matter\n- Meeting these assumptions helps ensure the validity of the F-test results. Violations of these assumptions can lead to incorrect conclusions, such as falsely detecting a difference in variances when none exists.\n",
      "metadata": {}
    },
    {
      "id": "0871cfd5-fe5c-4d89-9154-62b5263bc2a8",
      "cell_type": "markdown",
      "source": "##Question.4\n\n# Purpose of ANOVA and its Difference from a t-test\n\nBoth ANOVA and t-tests are statistical methods used to compare means, but they serve different purposes and are used in different contexts.\n\n## Purpose of ANOVA\n- **Objective**: ANOVA (Analysis of Variance) is used to compare the means of three or more groups to determine if there is a statistically significant difference among them.\n- **Application**: It is commonly used in experiments involving multiple groups or conditions, such as comparing the effectiveness of different treatments or products.\n- **How it Works**: ANOVA tests whether the variance between the group means is larger than the variance within the groups, indicating that at least one group mean is different from the others.\n\n## Purpose of t-test\n- **Objective**: A t-test is used to compare the means of two groups to determine if there is a statistically significant difference between them.\n- **Application**: It is typically used in simpler scenarios where only two groups or conditions are being compared.\n- **Types of t-tests**:\n  - **Independent t-test**: Compares the means of two independent groups.\n  - **Paired t-test**: Compares the means of two related groups, such as measurements before and after a treatment.\n\n## Key Differences\n1. **Number of Groups**:\n   - **ANOVA**: Used when comparing three or more groups.\n   - **t-test**: Used when comparing exactly two groups.\n\n2. **Complexity**:\n   - **ANOVA**: Suitable for complex experiments with multiple factors and levels.\n   - **t-test**: Best for simple comparisons between two groups.\n\n3. **Multiple Comparisons**:\n   - **ANOVA**: Reduces the risk of Type I error (false positives) when comparing multiple groups, by testing all group means simultaneously.\n   - **t-test**: If used repeatedly for multiple comparisons, increases the risk of Type I error.\n\n## Why Use ANOVA Over Multiple t-tests?\n- Using multiple t-tests increases the chance of incorrectly rejecting the null hypothesis. ANOVA controls this by testing all groups in a single analysis.\n",
      "metadata": {}
    },
    {
      "id": "a539cb78-9466-4d7e-a470-c03079b631f9",
      "cell_type": "markdown",
      "source": "##Question.5 \n\n# When and Why to Use One-Way ANOVA Instead of Multiple t-tests\n\n## When to Use One-Way ANOVA\n- **Scenario**: One-way ANOVA is used when you want to compare the means of three or more independent groups to see if there is a statistically significant difference among them.\n- **Example**: If you have three different teaching methods and you want to compare their effectiveness on student performance, you would use a one-way ANOVA.\n\n## Why Not Use Multiple t-tests?\n- **Increased Risk of Type I Error**: \n  - Conducting multiple t-tests increases the probability of making a Type I error (false positive), where you might incorrectly conclude that there is a difference when there isn't one.\n  - For each t-test conducted, there is a 5% chance (assuming a significance level of 0.05) of a Type I error. Conducting multiple t-tests increases this cumulative risk.\n\n## Why Use One-Way ANOVA?\n- **Controls Type I Error**: \n  - One-way ANOVA controls the overall Type I error rate by testing all group means simultaneously in a single analysis, rather than performing multiple separate tests.\n  \n- **Efficiency**: \n  - One-way ANOVA is more efficient as it provides a single test to determine whether there are any significant differences among the groups, without the need for multiple comparisons.\n  \n- **Comprehensive Analysis**:\n  - ANOVA can also be extended to more complex designs (e.g., factorial ANOVA) to analyze the effects of multiple factors on the outcome.\n\n## Conclusion\n- Use **one-way ANOVA** when comparing three or more groups to avoid the inflated Type I error rate that comes with multiple t-tests, and to gain a more efficient and comprehensive analysis.\n",
      "metadata": {}
    },
    {
      "id": "aa541d65-65b0-4112-afc6-34420664b2fe",
      "cell_type": "markdown",
      "source": "##Question.6\n\n\n# Partitioning of Variance in ANOVA and its Contribution to the F-statistic\n\nIn ANOVA, the total variance in the data is partitioned into two components: between-group variance and within-group variance. This partitioning helps in determining whether there are significant differences between the group means.\n\n## 1. Total Variance (SST)\n- **Total Sum of Squares (SST)**: It represents the total variance in the data, calculated as the sum of the squared differences between each observation and the overall mean.\n\n## 2. Between-Group Variance (SSB)\n- **Between-Group Sum of Squares (SSB)**: This measures the variance between the group means and the overall mean.\n  - It captures how much the group means differ from the overall mean.\n  - A larger between-group variance indicates that the group means are more spread out from the overall mean.\n\n## 3. Within-Group Variance (SSW)\n- **Within-Group Sum of Squares (SSW)**: This measures the variance within each group, calculated as the sum of the squared differences between each observation and its group mean.\n  - It captures the variation within each group.\n  - A smaller within-group variance indicates that the data points are closer to their respective group means.\n\n## 4. Calculation of the F-Statistic\n- **F-Statistic**: It is the ratio of the between-group variance to the within-group variance.\n  - **Formula**: F = (Between-group variance) / (Within-group variance)\n  - A large F-statistic suggests that the between-group variance is much greater than the within-group variance, indicating significant differences between the group means.\n\n## 5. Contribution to the F-test\n- **Hypothesis Testing**: \n  - Null Hypothesis (H₀): Assumes that all group means are equal, implying that the between-group variance should be similar to the within-group variance.\n  - Alternative Hypothesis (H₁): Assumes that at least one group mean is different, leading to a larger between-group variance compared to the within-group variance.\n  \n- **Interpretation**: \n  - If the F-statistic is significantly larger than 1, it indicates that the variance between the groups is greater than the variance within the groups, suggesting a significant difference in group means.\n",
      "metadata": {}
    },
    {
      "id": "17b76342-a922-4a16-ac27-1dc43cf9d7c3",
      "cell_type": "markdown",
      "source": "##Question.7\n\n# Comparison of Classical (Frequentist) and Bayesian Approach to ANOVA\n\nANOVA can be approached through both classical (frequentist) and Bayesian frameworks. These approaches differ in how they handle uncertainty, parameter estimation, and hypothesis testing.\n\n## 1. Handling of Uncertainty\n\n### Classical (Frequentist) Approach\n- **Uncertainty**: Uncertainty is handled using p-values and confidence intervals.\n- **Interpretation**: A p-value indicates the probability of observing the data, or something more extreme, assuming the null hypothesis is true.\n- **Focus**: Emphasizes long-run frequencies of outcomes. Uncertainty is about whether the sample results reflect the true population.\n\n### Bayesian Approach\n- **Uncertainty**: Uncertainty is quantified using probability distributions for the parameters.\n- **Interpretation**: Provides a posterior probability distribution, which directly quantifies the uncertainty about the parameter values after observing the data.\n- **Focus**: Uses prior information along with the observed data to update beliefs about the parameters.\n\n## 2. Parameter Estimation\n\n### Classical (Frequentist) Approach\n- **Estimation**: Parameters (e.g., group means) are estimated using sample data, typically via methods like least squares.\n- **Point Estimates**: Provides point estimates (e.g., sample means) and confidence intervals for parameters.\n- **Assumptions**: Relies on assumptions about the sampling distribution of the estimator.\n\n### Bayesian Approach\n- **Estimation**: Parameters are estimated by calculating the posterior distribution, which combines the prior distribution with the likelihood of the observed data.\n- **Credible Intervals**: Provides credible intervals, which indicate the range of parameter values with a specified probability.\n- **Flexibility**: Can incorporate prior knowledge or beliefs about the parameters.\n\n## 3. Hypothesis Testing\n\n### Classical (Frequentist) Approach\n- **Hypothesis Testing**: Uses p-values to test null hypotheses. Rejects the null hypothesis if the p-value is less than a predefined significance level (e.g., 0.05).\n- **Null Hypothesis**: Assumes no effect or no difference between groups. Results are based on the probability of observing the data under the null hypothesis.\n\n### Bayesian Approach\n- **Hypothesis Testing**: Uses Bayes factors to compare models or hypotheses. The Bayes factor quantifies the evidence for one hypothesis relative to another.\n- **Model Comparison**: Directly compares the probability of the data under different hypotheses, incorporating both the prior and observed data.\n\n## 4. Interpretation of Results\n\n### Classical (Frequentist) Approach\n- **P-values**: A small p-value suggests rejecting the null hypothesis, but it does not provide a direct probability about the hypothesis being true.\n- **Deterministic**: Once the null hypothesis is rejected or not, no further updating of beliefs is done.\n\n### Bayesian Approach\n- **Posterior Probabilities**: Provides a probability for the hypothesis or model being true, given the data.\n- **Iterative**: Can update the results as more data becomes available, refining the posterior distribution.\n\n## Conclusion\n- The **classical approach** focuses on long-run frequencies and p-values, offering a more binary decision-making process.\n- The **Bayesian approach** provides a more flexible framework, allowing for direct probability statements about parameters and hypotheses, and incorporating prior information to refine estimates.\n",
      "metadata": {}
    },
    {
      "id": "6e25221f-30d6-4a36-8c8f-a05735577389",
      "cell_type": "code",
      "source": "##Question.8\n\nimport numpy as np\nfrom scipy.stats import f\n\n# Data for Profession A and B\nprofession_A = [48, 52, 55, 60, 62]\nprofession_B = [45, 50, 55, 52, 47]\n\n# Calculate variances\nvariance_A = np.var(profession_A, ddof=1)  # Variance of Profession A\nvariance_B = np.var(profession_B, ddof=1)  # Variance of Profession B\n\n# Calculate the F-statistic\nF_statistic = variance_A / variance_B\n\n# Degrees of freedom\ndf1 = len(profession_A) - 1  # Degrees of freedom for Profession A\ndf2 = len(profession_B) - 1  # Degrees of freedom for Profession B\n\n# Calculate the p-value\np_value_one_tailed = f.cdf(F_statistic, df1, df2)  # One-tailed p-value\np_value_two_tailed = 1 - p_value_one_tailed  # Two-tailed p-value\n\n# Output the results\nprint(\"F-statistic:\", F_statistic)\nprint(\"p-value (one-tailed):\", p_value_one_tailed)\nprint(\"p-value (two-tailed):\", p_value_two_tailed)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "F-statistic: 2.089171974522293\np-value (one-tailed): 0.7534757004973305\np-value (two-tailed): 0.24652429950266952\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "1147ad0d-0c9b-40ef-ad19-77bacdc94464",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}